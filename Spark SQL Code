import findspark
findspark.init()


import sys

from pyspark import SparkContext
from pyspark.sql import SparkSession, Row

sc = SparkContext(appName="sga")
spark = SparkSession(sc)


df = se.read.csv('hdfs:/data/clickstream.csv', header=True, sep = '\t')
df.registerTempTable("df")


result = spark.sql("""
select route, count(*) as count from

(select user_id, session_id, array_join(collect_list(event_page),'-') AS route
from

(select *, 
max(error_time) over (partition by user_id, session_id order by timestamp, event_page) as error_time_in_group
from

(select *, 
case when event_type like '%error%' then timestamp else 0 end as error_time, 
LAG(event_page, 1, 'nothing') OVER (partition by user_id, session_id order by timestamp, event_page) as preveous_page
from df))

where error_time_in_group == 0 
and event_page != preveous_page
group by user_id, session_id)

group by route
order by count desc
limit 30
""")


result.show(30)
